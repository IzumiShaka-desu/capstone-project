{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to create the Image Augmentation\n",
    "training_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip='True')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10901 images belonging to 6 classes.\n",
      "Found 2698 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = training_datagen.flow_from_directory('archive/dataset/train', batch_size=60, target_size=(64,64), class_mode='categorical')\n",
    "test_set = test_datagen.flow_from_directory('archive/dataset/test',batch_size=60, target_size=(64,64),class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 60, 60, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 30, 30, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 26, 26, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 10816)             0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 10816)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1024)              11076608  \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 521)               534025    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 3132      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,679,333\n",
      "Trainable params: 11,679,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# 1st layer\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), input_shape=[64,64,3], activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
    "# 2nd layer\n",
    "model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2),strides=2))\n",
    "# flatten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# neural network\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(units=1024,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=521, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=6, activation='softmax'))\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/media/tedd404/My Passport/Bangkit/Capstone/Food_Security.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/tedd404/My%20Passport/Bangkit/Capstone/Food_Security.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39m#compling\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/tedd404/My%20Passport/Bangkit/Capstone/Food_Security.ipynb#ch0000004?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/tedd404/My%20Passport/Bangkit/Capstone/Food_Security.ipynb#ch0000004?line=2'>3</a>\u001b[0m history\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtraining_set,validation_data\u001b[39m=\u001b[39;49mtest_set,epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/media/tedd404/My Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/media/tedd404/My Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:111\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=108'>109</a>\u001b[0m     color_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=109'>110</a>\u001b[0m \u001b[39mif\u001b[39;00m pil_image \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=110'>111</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not import PIL.Image. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=111'>112</a>\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mThe use of `load_img` requires PIL.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=112'>113</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='file:///media/tedd404/My%20Passport/Bangkit/Capstone/Capstone/lib/python3.8/site-packages/keras_preprocessing/image/utils.py?line=113'>114</a>\u001b[0m     img \u001b[39m=\u001b[39m pil_image\u001b[39m.\u001b[39mopen(io\u001b[39m.\u001b[39mBytesIO(f\u001b[39m.\u001b[39mread()))\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "#compling\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "history=model.fit(x=training_set,validation_data=test_set,epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='bottom right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('Model_Uji_Coba.h5')\n",
    "model_json = model.to_json()\n",
    "with open(\"Model_Uji_Coba.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "np.loadtxt\n",
    "classes = ['Fresh Apple','Fresh Banana','Fresh Orange','Rotten Apple','Rotten Banana','Rotten Orange']#creating the class labels\n",
    "test_image = image.load_img('tes_coba/', target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "print(np.argmax(result))\n",
    "result=result[0]\n",
    "for i in range(6):\n",
    "    if result[i] == 1.:\n",
    "        break;\n",
    "prediction = classes[i]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving in array #bypass \n",
    "import numpy as np\n",
    "labels=training_datagen.flow_from_directory('/content/dataset/train')\n",
    "Demo_labels=labels.class_indices\n",
    "print(Demo_labels)\n",
    "print(\"Dict data is converted to array\")\n",
    "#converting dict to array\n",
    "data = list(dict. items(Demo_labels))\n",
    "an_array = np. array(data)\n",
    "print(an_array)\n",
    "np.save('Fruits_labels',an_array)\n",
    "#saving in array form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction from array #bypass\n",
    "label_data=np.load('/content/Fruits_labels.npy')\n",
    "test_image = image.load_img('/content/apple.jpeg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "result1=result[0]\n",
    "for i in range(6):\n",
    "  if result1[i] == 1.:\n",
    "    break;\n",
    "prediction = label_data[i][0]\n",
    "print(prediction) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baf5d31a1b565f1a3bb9e1a5312398df6838d4de5d69a1c3fec78c534c3f2bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Capstone': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
